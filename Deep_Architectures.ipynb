{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Architectures.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lwhluvdemo/mydeeplearning/blob/master/Deep_Architectures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "f3YWYg936LPq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B6ZrTfkk6cLR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NUM_INPUTS=100\n",
        "HIDDEN_SIZE=1024\n",
        "NUM_OUTPUTS=20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OqD5-YhaMl-s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Logistic Regresion"
      ]
    },
    {
      "metadata": {
        "id": "NcwnkeQw6SAC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lor = nn.Sequential(\n",
        "    nn.Linear(NUM_INPUTS, 1),\n",
        "    nn.Sigmoid()\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AqnLN0ruMto0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.Linear Regresion"
      ]
    },
    {
      "metadata": {
        "id": "D0Bsumng65VV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lir = nn.Sequential(\n",
        "    nn.Linear(NUM_INPUTS, 1)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iWPTUONOMxp6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.  Softmax classifier"
      ]
    },
    {
      "metadata": {
        "id": "QnaTs6dZ8BMe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "smx = nn.Sequential(\n",
        "    nn.Linear(NUM_INPUTS, NUM_OUTPUTS),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "550SLZ4eM1pu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4. MultiLayer Perceptron"
      ]
    },
    {
      "metadata": {
        "id": "2lTm0pcd_UzE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NUM_INPUTS=100\n",
        "HIDDEN_SIZE=1024\n",
        "NUM_OUTPUTS=20\n",
        "\n",
        "mlp = nn.Sequential(\n",
        "    nn.Linear(NUM_INPUTS, HIDDEN_SIZE),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(HIDDEN_SIZE, NUM_OUTPUTS),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hYHX1T_iM4IN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5. Embedding with fully connected layer"
      ]
    },
    {
      "metadata": {
        "id": "nFtBPGJ7Cc0j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 10000\n",
        "HIDDEN_SIZE=100\n",
        "# mapping a Vocabulary of size 10.000 to HIDDEN_SIZE projections\n",
        "emb_1 = nn.Linear(VOCAB_SIZE, HIDDEN_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cj_EhCFcFrYw",
        "colab_type": "code",
        "outputId": "a8f8b072-e23f-4e86-dbf7-b46b6fddae3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# forward example [10, 10000] tensor\n",
        "code = [1] + [0] * 9999\n",
        "# copy 10 times the same code [1 0 0 0 ... 0]\n",
        "x = torch.FloatTensor([code] * 10)\n",
        "print('Input x tensor size: ', x.size())\n",
        "y = emb_1(x)\n",
        "print('Output y embedding size: ', y.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input x tensor size:  torch.Size([10, 10000])\n",
            "Output y embedding size:  torch.Size([10, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XxmC8kTWM-wV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6. Embedding with Embedding layer"
      ]
    },
    {
      "metadata": {
        "id": "U8090BmHER9b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 10000\n",
        "HIDDEN_SIZE=100\n",
        "# mapping a Vocabulary of size 10.000 to HIDDEN_SIZE projections\n",
        "emb_2 = nn.Embedding(VOCAB_SIZE, HIDDEN_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xIQmFOqUFCM_",
        "colab_type": "code",
        "outputId": "a537d62f-e28a-4dcc-c158-1f979c5be241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Just make a long tensor with zero-index\n",
        "x = torch.zeros(10, 1).long()\n",
        "print('Input x tensor size: ', x.size())\n",
        "y = emb_2(x)\n",
        "print('Output y embedding size: ', y.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input x tensor size:  torch.Size([10, 1])\n",
            "Output y embedding size:  torch.Size([10, 1, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8XyCuG_1NAj3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 7. Recurrent Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "Ir2jGezQFHzC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NUM_INPUTS = 100\n",
        "HIDDEN_SIZE = 512\n",
        "NUM_LAYERS = 1\n",
        "# define a recurrent layer\n",
        "rnn = nn.RNN(NUM_INPUTS, HIDDEN_SIZE, num_layers=NUM_LAYERS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QwrtfxJXVxpR",
        "colab_type": "code",
        "outputId": "bd16be2b-350e-47de-fb17-217d3f54d82d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "SEQ_LEN = 100\n",
        "x = torch.randn(SEQ_LEN, 1, NUM_INPUTS)\n",
        "print('Input tensor size [seq_len, bsize, hidden_size]: ', x.size())\n",
        "ht, state = rnn(x, None)\n",
        "print('Output tensor h[t] size [seq_len, bsize, hidden_size]: ', ht.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input tensor size [seq_len, bsize, hidden_size]:  torch.Size([100, 1, 100])\n",
            "Output tensor h[t] size [seq_len, bsize, hidden_size]:  torch.Size([100, 1, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eZxdVYlqWRcH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NUM_INPUTS = 100\n",
        "HIDDEN_SIZE = 512\n",
        "NUM_LAYERS = 1\n",
        "# define a recurrent layer, swapping batch and time axis\n",
        "rnn = nn.RNN(NUM_INPUTS, HIDDEN_SIZE, num_layers=NUM_LAYERS,\n",
        "            batch_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gbiNHH0zWrr-",
        "colab_type": "code",
        "outputId": "3b480387-0d9e-441a-8c53-ce171faab8a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "SEQ_LEN = 100\n",
        "x = torch.randn(1, SEQ_LEN, NUM_INPUTS)\n",
        "print('Input tensor size [bsize, seq_len, hidden_size]: ', x.size())\n",
        "ht, state = rnn(x, None)\n",
        "print('Output tensor h[t] size [bsize, seq_len, hidden_size]: ', ht.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input tensor size [bsize, seq_len, hidden_size]:  torch.Size([1, 100, 100])\n",
            "Output tensor h[t] size [bsize, seq_len, hidden_size]:  torch.Size([1, 100, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7TZ0bj9wWw0Z",
        "colab_type": "code",
        "outputId": "ac1aebfa-da0f-4e14-c8bb-cad4f6ccefc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# let's check ht and state sizes\n",
        "print('ht size: ', ht.size())\n",
        "print('state size: ', state.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ht size:  torch.Size([1, 100, 512])\n",
            "state size:  torch.Size([1, 1, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "95ioy0LAXMUN",
        "colab_type": "code",
        "outputId": "78c2d160-be2c-46e6-cd43-dd7e8a68cbf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "NUM_INPUTS = 100\n",
        "NUM_OUTPUTS = 10\n",
        "HIDDEN_SIZE = 512\n",
        "SEQ_LEN = 100\n",
        "NUM_LAYERS = 1\n",
        "# define a recurrent layer, swapping batch and time axis and connect\n",
        "# an FC layer as an output layer to build a full network\n",
        "rnn = nn.RNN(NUM_INPUTS, HIDDEN_SIZE, num_layers=NUM_LAYERS,\n",
        "            batch_first=True)\n",
        "fc = nn.Sequential(\n",
        "    nn.Linear(HIDDEN_SIZE, NUM_OUTPUTS),\n",
        "    nn.LogSoftmax(dim=2)\n",
        ")\n",
        "\n",
        "x = torch.randn(1, SEQ_LEN, NUM_INPUTS)\n",
        "print('Input tensor size x: ', x.size())\n",
        "ht, state = rnn(x, None)\n",
        "print('Hidden tensor size ht: ', ht.size())\n",
        "y = fc(ht)\n",
        "print('Output tensor y size: ', y.size())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input tensor size x:  torch.Size([1, 100, 100])\n",
            "Hidden tensor size ht:  torch.Size([1, 100, 512])\n",
            "Output tensor y size:  torch.Size([1, 100, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "shvFLclDNFDt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 8. LSTM Recurrent Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "DDzAWGL5ZEQ5",
        "colab_type": "code",
        "outputId": "5eaadaac-4dd3-42b8-d210-0cbc2cce9aab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "lstm = nn.LSTM(NUM_INPUTS, HIDDEN_SIZE, num_layers=NUM_LAYERS,\n",
        "              batch_first=True)\n",
        "x = torch.randn(1, SEQ_LEN, NUM_INPUTS)\n",
        "print('Input tensor size x: ', x.size())\n",
        "ht, states = lstm(x, None)\n",
        "hT, cT = states[0], states[1]\n",
        "print('Output tensor ht size: ', ht.size())\n",
        "print('Last state h[T]: ', hT.size())\n",
        "print('Cell state c[T]: ', cT.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input tensor size x:  torch.Size([1, 100, 100])\n",
            "Output tensor ht size:  torch.Size([1, 100, 512])\n",
            "Last state h[T]:  torch.Size([1, 1, 512])\n",
            "Cell state c[T]:  torch.Size([1, 1, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-G8eHj4bNIt9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 9. Convolutional Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "hVv1Q-yvcvNj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NUM_CHANNELS_IN = 1\n",
        "HIDDEN_SIZE = 1024\n",
        "KERNEL_WIDTH = 3\n",
        "# Build a one-dimensional convolutional neural layer\n",
        "conv1d = nn.Conv1d(NUM_CHANNELS_IN, HIDDEN_SIZE, KERNEL_WIDTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UNInGheJef65",
        "colab_type": "code",
        "outputId": "b9435d6a-d09b-4479-b542-a5e25b68cd0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "SEQ_LEN = 8\n",
        "x = torch.randn(1, NUM_CHANNELS_IN, SEQ_LEN)\n",
        "print('Input tensor size x: ', x.size())\n",
        "y = conv1d(x)\n",
        "print('Output tensor y size: ', y.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input tensor size x:  torch.Size([1, 1, 8])\n",
            "Output tensor y size:  torch.Size([1, 1024, 6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EmJ5o59BetZb",
        "colab_type": "code",
        "outputId": "d1ee1101-fbd5-48a0-8bf1-b2991d43b85a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "NUM_CHANNELS_IN = 1\n",
        "HIDDEN_SIZE = 1024\n",
        "KERNEL_WIDTH = 3\n",
        "PADDING = KERNEL_WIDTH // 2 # = 1\n",
        "# Build a one-dimensional convolutional neural layer\n",
        "conv1d = nn.Conv1d(NUM_CHANNELS_IN, HIDDEN_SIZE, KERNEL_WIDTH, \n",
        "                   padding=PADDING)\n",
        "\n",
        "SEQ_LEN = 8\n",
        "x = torch.randn(1, NUM_CHANNELS_IN, SEQ_LEN)\n",
        "print('Input tensor size x: ', x.size())\n",
        "y = conv1d(x)\n",
        "print('Output tensor y size: ', y.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input tensor size x:  torch.Size([1, 1, 8])\n",
            "Output tensor y size:  torch.Size([1, 1024, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uZnAIecIfr3H",
        "colab_type": "code",
        "outputId": "affa12f5-7ca6-4b63-8423-e402953bba42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "NUM_CHANNELS_IN = 1\n",
        "HIDDEN_SIZE = 1024\n",
        "KERNEL_WIDTH = 3\n",
        "# Build a one-dimensional convolutional neural layer\n",
        "conv1d = nn.Conv1d(NUM_CHANNELS_IN, HIDDEN_SIZE, KERNEL_WIDTH)\n",
        "                   \n",
        "SEQ_LEN = 8\n",
        "PADDING = KERNEL_WIDTH - 1 # = 2\n",
        "x = torch.randn(1, NUM_CHANNELS_IN, SEQ_LEN)\n",
        "print('Input tensor x size: ', x.size())\n",
        "xpad = F.pad(x, (PADDING, 0))\n",
        "print('Input tensor after padding xpad size: ', xpad.size())\n",
        "y = conv1d(xpad)\n",
        "print('Output tensor y size: ', y.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input tensor x size:  torch.Size([1, 1, 8])\n",
            "Input tensor after padding xpad size:  torch.Size([1, 1, 10])\n",
            "Output tensor y size:  torch.Size([1, 1024, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cz2NAKOTNOMV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 10. Convolutional Neural Network as an MLP"
      ]
    },
    {
      "metadata": {
        "id": "QdAP5a4qgnLV",
        "colab_type": "code",
        "outputId": "3f3129bf-60b3-4593-ae14-58800d166ce5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "NUM_INPUTS = 100\n",
        "HIDDEN_SIZE = 1024\n",
        "NUM_OUTPUTS= 20\n",
        "# MLP as a CNN\n",
        "mlp = nn.Sequential(\n",
        "    nn.Conv1d(NUM_INPUTS, HIDDEN_SIZE, 1),\n",
        "    nn.Tanh(),\n",
        "    nn.Conv1d(HIDDEN_SIZE, HIDDEN_SIZE, 1),\n",
        "    nn.Tanh(),\n",
        "    nn.Conv1d(HIDDEN_SIZE, NUM_OUTPUTS, 1),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")\n",
        "\n",
        "x = torch.randn(1, 100, 1)\n",
        "print('Input tensor x size: ', x.size())\n",
        "y = mlp(x)\n",
        "print('Output tensor y size: ', y.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input tensor x size:  torch.Size([1, 100, 1])\n",
            "Output tensor y size:  torch.Size([1, 20, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "31XMMrLzNQoV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 11. Deconvolutional Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "_yOUif_eiNhF",
        "colab_type": "code",
        "outputId": "715a1cf3-5a1f-4c61-dcdf-b2f77677e137",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "NUM_CHANNELS_IN = 1\n",
        "HIDDEN_SIZE = 1\n",
        "KERNEL_WIDTH = 8\n",
        "STRIDE = 4\n",
        "\n",
        "deconv = nn.ConvTranspose1d(NUM_CHANNELS_IN, HIDDEN_SIZE, KERNEL_WIDTH,\n",
        "                            stride=STRIDE)\n",
        "\n",
        "SEQ_LEN = 2\n",
        "y = torch.randn(1, NUM_CHANNELS_IN, SEQ_LEN)\n",
        "print('Input tensor y size: ', y.size())\n",
        "x = deconv(y)\n",
        "print('Output (interpolated) tensor x size: ', x.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input tensor y size:  torch.Size([1, 1, 2])\n",
            "Output (interpolated) tensor x size:  torch.Size([1, 1, 12])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2KAdi7kwNbxt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 12. Quasi Recurrent Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "HlnG0POXlfTP",
        "colab_type": "code",
        "outputId": "a7900e10-d6f4-4265-a67c-4f0b22e84d41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "class fQRNNLayer(nn.Module):\n",
        "  \n",
        "  def __init__(self, num_inputs, num_outputs,\n",
        "              kwidth=2):\n",
        "    super().__init__()\n",
        "    self.num_inputs = num_inputs\n",
        "    self.num_outputs = num_outputs\n",
        "    self.kwidth = kwidth\n",
        "    # double feature maps for zt and ft predictions with same conv layer\n",
        "    self.conv = nn.Conv1d(num_inputs, num_outputs * 2, kwidth)\n",
        "    \n",
        "  def forward(self, x, state=None):\n",
        "    # x is [bsz, seq_len, num_inputs]\n",
        "    # state is [bsz, num_outputs] dimensional\n",
        "    # ---------- FEED FORWARD PART\n",
        "    # inference convolutional part\n",
        "    # transpose x axis first to work with CNN layer\n",
        "    x = x.transpose(1, 2)\n",
        "    pad = self.kwidth - 1\n",
        "    xp = F.pad(x, (pad, 0))\n",
        "    conv_h = self.conv(xp)\n",
        "    # split convolutional layer feature maps into zt (new state\n",
        "    # candidate) and forget activation ft\n",
        "    zt, ft = torch.chunk(conv_h, 2, dim=1)\n",
        "    # Convert forget gate into actual forget\n",
        "    ft = torch.sigmoid(ft)\n",
        "    # Convert zt into actual non-linear response\n",
        "    zt = torch.tanh(zt)\n",
        "    # ---------- SEQUENTIAL PART\n",
        "    # iterate through time now to make pooling\n",
        "    seqlen = ft.size(2)\n",
        "    if state is None:\n",
        "      # create the zero state\n",
        "      ht_1 = torch.zeros(ft.size(0), self.num_outputs, 1)\n",
        "    else:\n",
        "      # add the dim=2 to match 3D tensor shape\n",
        "      ht_1 = state.unsqueeze(2)\n",
        "    zts = torch.chunk(zt, zt.size(2), dim=2)\n",
        "    fts = torch.chunk(ft, ft.size(2), dim=2)\n",
        "    hts = []\n",
        "    for t in range(seqlen):\n",
        "      ht = ht_1 * fts[t] + (1 - fts[t]) * zts[t]\n",
        "      # transpose time, channels dims again to match RNN-like shape\n",
        "      hts.append(ht.transpose(1, 2))\n",
        "      # re-assign h[t-1] now\n",
        "      ht_1 = ht\n",
        "    # convert hts list into a 3D tensor [bsz, seq_len, num_outputs]\n",
        "    hts = torch.cat(hts, dim=1)\n",
        "    return hts, ht_1.squeeze(2)\n",
        "      \n",
        "    \n",
        "      \n",
        "fqrnn = fQRNNLayer(1, 100, 2)\n",
        "x = torch.randn(1, 10, 1)\n",
        "ht, state = fqrnn(x)\n",
        "print('ht size: ', ht.size())\n",
        "print('state size: ', state.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ht size:  torch.Size([1, 10, 100])\n",
            "state size:  torch.Size([1, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jyks-SaXNgTl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 13. AlexNet classifier"
      ]
    },
    {
      "metadata": {
        "id": "kI18xwc2pjnk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), 256 * 6 * 6)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sM8v8GrFvwmw",
        "colab_type": "code",
        "outputId": "f65b90c5-538c-43f1-df2c-a0a5983f8a4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "alexnet = AlexNet()\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "print('Input tensor x size: ', x.size())\n",
        "y = alexnet(x)\n",
        "print('Output tensor y size: ', y.size())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input tensor x size:  torch.Size([1, 3, 224, 224])\n",
            "Output tensor y size:  torch.Size([1, 1000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PlRjHUfSNkVH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 14. Residual connections"
      ]
    },
    {
      "metadata": {
        "id": "5kGF6Ysqwiit",
        "colab_type": "code",
        "outputId": "7ce59005-df36-44b4-a860-e2c340475ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "class ResLayer(nn.Module):\n",
        "  \n",
        "  def __init__(self, num_inputs):\n",
        "    super().__init__()\n",
        "    self.num_inputs = num_inputs\n",
        "    num_outputs = num_inputs\n",
        "    self.num_outputs = num_outputs\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(num_inputs, num_outputs, 3, padding=1),\n",
        "        nn.BatchNorm2d(num_outputs),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.Conv2d(num_outputs, num_outputs, 3, padding=1),\n",
        "        nn.BatchNorm2d(num_outputs),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "    self.out_relu = nn.ReLU(inplace=True)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # non-linear processing trunk\n",
        "    conv1_h = self.conv1(x)\n",
        "    conv2_h = self.conv2(conv1_h)\n",
        "    # output is result of res connection + non-linear processing\n",
        "    y = self.out_relu(x + conv2_h)\n",
        "    return y\n",
        "    \n",
        "x = torch.randn(1, 64, 100, 100)\n",
        "print('Input tensor x size: ', x.size())\n",
        "reslayer = ResLayer(64)\n",
        "y = reslayer(x)\n",
        "print('Output tensor y size: ', y.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input tensor x size:  torch.Size([1, 64, 100, 100])\n",
            "Output tensor y size:  torch.Size([1, 64, 100, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "__E0NQb9N2lW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 15. Auto-Encoder Network"
      ]
    },
    {
      "metadata": {
        "id": "eF7gYhpNz3Q8",
        "colab_type": "code",
        "outputId": "ba362673-ce4a-4f15-d44b-478c8498798c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "class AE(nn.Module):\n",
        "    def __init__(self, num_inputs=784):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(num_inputs, 400),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(400, 400),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(400, 20)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(20, 400),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(400, 400),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(400, num_inputs)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(self.encoder(x))\n",
        "\n",
        "      \n",
        "ae = AE(784)\n",
        "x = torch.randn(10, 784)\n",
        "print('Input tensor x size: ', x.size())\n",
        "y = ae(x)\n",
        "print('Output tensor y size: ', y.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input tensor x size:  torch.Size([10, 784])\n",
            "Output tensor y size:  torch.Size([10, 784])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "anEbOUKTN-1H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 16. Variational Auto-Encoder Network"
      ]
    },
    {
      "metadata": {
        "id": "drIFisw02OqR",
        "colab_type": "code",
        "outputId": "3c5cb817-0109-49b8-f02f-c20e28a36ddc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "# from https://github.com/pytorch/examples/blob/master/vae/main.py\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)\n",
        "        self.fc22 = nn.Linear(400, 20)\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "vae = VAE()\n",
        "x = torch.randn(10, 784)\n",
        "print('Input tensor x size: ', x.size())\n",
        "y, mu, logvar = vae(x)\n",
        "print('Input tensor y size: ', y.size())\n",
        "print('Mean tensor mu size: ', mu.size())\n",
        "print('Covariance tensor logvar size: ', logvar.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input tensor x size:  torch.Size([10, 784])\n",
            "Input tensor y size:  torch.Size([10, 784])\n",
            "Mean tensor mu size:  torch.Size([10, 20])\n",
            "Covariance tensor logvar size:  torch.Size([10, 20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pxGOvdLVOBKk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 17. Deep Convolutional Auto-Encoder with skip connections (SEGAN G)"
      ]
    },
    {
      "metadata": {
        "id": "rnHMfxwKA65l",
        "colab_type": "code",
        "outputId": "6f86b706-0fa6-4d89-e113-f77f70bd5ccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "class DownConv1dBlock(nn.Module):\n",
        "  \n",
        "  def __init__(self, ninp, fmap, kwidth, stride):\n",
        "    super().__init__()\n",
        "    assert stride > 1, stride\n",
        "    self.kwidth = kwidth\n",
        "    self.conv = nn.Conv1d(ninp, fmap, kwidth, stride=stride)\n",
        "    self.act = nn.ReLU(inplace=True)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # calculate padding with stride > 1\n",
        "    pad_left = self.kwidth // 2 - 1\n",
        "    pad_right = self.kwidth // 2\n",
        "    xp = F.pad(x, (pad_left, pad_right))\n",
        "    y = self.act(self.conv(xp))\n",
        "    return y\n",
        "\n",
        "block = DownConv1dBlock(1, 1, 31, 4)\n",
        "x = torch.randn(1, 1, 4000)\n",
        "print('Input tensor x size: ', x.size())\n",
        "y = block(x)\n",
        "print('Output tensor y size: ', y.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input tensor x size:  torch.Size([1, 1, 4000])\n",
            "Output tensor y size:  torch.Size([1, 1, 1000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aJHX1W_VA7v0",
        "colab_type": "code",
        "outputId": "318582ba-7256-4d10-b544-47d137c4d3bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "class UpConv1dBlock(nn.Module):\n",
        "  \n",
        "  def __init__(self, ninp, fmap, kwidth, stride, act=True):\n",
        "    super().__init__()\n",
        "    assert stride > 1, stride\n",
        "    self.kwidth = kwidth\n",
        "    pad = max(0, (stride - kwidth) // -2)\n",
        "    self.deconv = nn.ConvTranspose1d(ninp, fmap, kwidth,\n",
        "                                    stride=stride,\n",
        "                                    padding=pad)\n",
        "    if act:\n",
        "      self.act = nn.ReLU(inplace=True)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h = self.deconv(x)\n",
        "    if self.kwidth % 2 != 0:\n",
        "      # drop last item for shape compatibility with TensorFlow deconvs\n",
        "      h = h[:, :, :-1]\n",
        "    if hasattr(self, 'act'):\n",
        "      y = self.act(h)\n",
        "    else:\n",
        "      y = h\n",
        "    return y\n",
        "\n",
        "block = UpConv1dBlock(1, 1, 31, 4)\n",
        "x = torch.randn(1, 1, 1000)\n",
        "print('Input tensor x size: ', x.size())\n",
        "y = block(x)\n",
        "print('Output tensor y size: ', y.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input tensor x size:  torch.Size([1, 1, 1000])\n",
            "Output tensor y size:  torch.Size([1, 1, 4000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rYw0SPRd3GRs",
        "colab_type": "code",
        "outputId": "c5b8b87e-c006-4bd2-ce2c-2ff90c036534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "class Conv1dGenerator(nn.Module):\n",
        "  \n",
        "  def __init__(self, enc_fmaps=[64, 128, 256, 512], kwidth=31,\n",
        "               pooling=4):\n",
        "    super().__init__()\n",
        "    self.enc = nn.ModuleList()\n",
        "    ninp = 1\n",
        "    for enc_fmap in enc_fmaps:\n",
        "      self.enc.append(DownConv1dBlock(ninp, enc_fmap, kwidth, pooling))\n",
        "      ninp = enc_fmap\n",
        "    \n",
        "    self.dec = nn.ModuleList()\n",
        "    # revert encoder feature maps\n",
        "    dec_fmaps = enc_fmaps[::-1][1:] + [1]\n",
        "    act = True\n",
        "    for di, dec_fmap in enumerate(dec_fmaps, start=1):\n",
        "      if di >= len(dec_fmaps):\n",
        "        # last decoder layer has no activation\n",
        "        act = False\n",
        "      self.dec.append(UpConv1dBlock(ninp, dec_fmap, kwidth, pooling, act=act))\n",
        "      ninp = dec_fmap\n",
        "  \n",
        "  def forward(self, x):\n",
        "    skips = []\n",
        "    h = x\n",
        "    for ei, enc_layer in enumerate(self.enc, start=1):\n",
        "      h = enc_layer(h)\n",
        "      if ei < len(self.enc):\n",
        "        skips.append(h)\n",
        "    # now decode\n",
        "    \n",
        "    for di, dec_layer in enumerate(self.dec, start=1):\n",
        "      if di > 1:\n",
        "        # sum skip connection\n",
        "        skip_h = skips.pop(-1)\n",
        "        h = h + skip_h\n",
        "      h = dec_layer(h)\n",
        "    y = h\n",
        "    return y\n",
        "      \n",
        "G = Conv1dGenerator()\n",
        "x = torch.randn(1, 1, 8192)\n",
        "print('Input tensor x size: ', x.size())\n",
        "y = G(x)\n",
        "print('Output tensor y size: ', y.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input tensor x size:  torch.Size([1, 1, 8192])\n",
            "Output tensor y size:  torch.Size([1, 1, 8192])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R0yipK1wOKIE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 18. DCGAN G and D"
      ]
    },
    {
      "metadata": {
        "id": "dbXceSoO6g3l",
        "colab_type": "code",
        "outputId": "0802ee5d-25df-488f-aeea-f73c468f51ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# from https://github.com/pytorch/examples/blob/master/dcgan/main.py\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, nc=3):\n",
        "        super().__init__()\n",
        "        nz = 100\n",
        "        ngf = 64\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "      return self.main(input)\n",
        "\n",
        "z = torch.randn(1, 100, 1, 1)\n",
        "print('Input tensor z size: ', z.size())\n",
        "G = Generator()\n",
        "x = G(z)\n",
        "print('Output tensor x size: ', x.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input tensor z size:  torch.Size([1, 100, 1, 1])\n",
            "Output tensor x size:  torch.Size([1, 3, 64, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7xuh9ww3Inea",
        "colab_type": "code",
        "outputId": "4ea7940e-af66-48d9-dca0-c21f54131150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, nc=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "        ndf = 64\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "      \n",
        "      \n",
        "x = torch.randn(1, 3, 64, 64)\n",
        "print('Input tensor x size: ', x.size())\n",
        "D = Discriminator()\n",
        "y = D(x)\n",
        "print('Output tensor y size: ', y.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input tensor x size:  torch.Size([1, 3, 64, 64])\n",
            "Output tensor y size:  torch.Size([1, 1, 1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0IsQDwR5J1Ey",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}